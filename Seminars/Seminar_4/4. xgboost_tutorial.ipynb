{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг на решающих деревьях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='blue'>XGBoost</font> eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Установка](https://xgboost.readthedocs.io/en/latest/build.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет Boston Housing и обучим XGBoost на нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "boston = load_boston()\n",
    "y = #выделите целевой вектор\n",
    "X = # выделите объекты и признаки\n",
    "\n",
    "kf = KFold(y.shape[0], shuffle=True, random_state=rng)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преимущества:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* потенциально очень высокое качество во многих задачах\n",
    "\n",
    "* находит нелинейные связи\n",
    "\n",
    "* способен обработать датасеты с большим числом объектов и признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недостатки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* очень много параметров\n",
    "\n",
    "* модели не интерпретируемы\n",
    "\n",
    "* по умолчанию не очень быстрый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost предлагает 2 способа использования алгоритмов:\n",
    "* sklearn-совместимые классы XGBClassifier, XGBRegressor\n",
    "\n",
    "* \"оригинальная\" python-библиотека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"\n",
    "    params[\"booster\"] = \"gbtree\"\n",
    "    params[\"eval_metric\"] = \"rmse\"\n",
    "    params[\"num_boost_round\"] = 100\n",
    "    params[\"max_depth\"] = 3\n",
    "    params[\"tree_method\"] = \"approx\"\n",
    "    params[\"sketch_eps\"] = 1\n",
    "    \n",
    "    return params\n",
    "    \n",
    "for train_index, test_index in kf.split(X):\n",
    "\n",
    "    params = #получите параметры\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = #передайте параметры и обучающую выборку и обучите регрессор\n",
    "\n",
    "    print (\"RMSE on fold {}: {}\".format(train_index, bst.eval(xgtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особенности XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Написан на C++, есть обертки на Python, R, Java, Scala\n",
    "\n",
    "С помощью XGBoost выиграна половина конкурсов на Kaggle\n",
    "\n",
    "Существует коммерческая версия TreeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Для уменьшения переобучения целевая функция поддерживает L0, L1, L2 регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Параллелизм (по признакам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Также есть возможность запускаться на Hadoop, Spark, Flink и DataFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные функции потерь / метрики качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В XGBoost встроено множество различных функций потерь:\n",
    "\n",
    "* reg:linear\n",
    "\n",
    "* reg:logistic\n",
    "\n",
    "* binary:logistic\n",
    "\n",
    "* binary:logitraw\n",
    "\n",
    "* multi:softmax\n",
    "\n",
    "* rank:pairwise\n",
    "\n",
    "* ...\n",
    "\n",
    "А также соответствующих eval_metric, которые замеряют качество и позволяют сделать early stop.\n",
    "\n",
    "Но также имеется возможность реализовать свой objective и eval_metric.\n",
    "\n",
    "Все, что для этого нужно - уметь считать градиент и гессиан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reg_linear(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    grad = (preds - labels)\n",
    "    hess = np.ones(labels.shape[0])\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    params = #получите параметры\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = #обучите регрессор и передайте в train в качестве obg my_reg_linear\n",
    "    \n",
    "    predictions = #постройте предсказания\n",
    "    actuals = y[test_index]\n",
    "\n",
    "    print (bst.eval(xgtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximated tree splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данных слишком много, то можно использовать не все значения признаков, а разделить их на бакеты.\n",
    "\n",
    "А именно, от каждого признака берутся не все значения, а только некоторое подмножество. Разбиение производится по элементам этого подмножества. \n",
    "\n",
    "Для разбиения выбираются взвешенные перцентили.\n",
    "\n",
    "В оригинальной статье указывается 2 алгоритма:\n",
    "*   глобальный - один раз выбрать разбиение значений фактора перед началом построения дерева и зафиксировать\n",
    "\n",
    "    экономим на выборе разбиений, но обычно приходится выбирать больше точек разбиения\n",
    "    \n",
    "    \n",
    "*   локальный - выбирать разбиение после каждого сплита\n",
    "  \n",
    "    работает лучше на глубоких деревьях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"tree_method\"] = \"approx\"\n",
    "params[\"sketch_eps\"] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пропуски в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost умеет обрабатывать разреженные матрицы\n",
    "\n",
    "Но категориальные признаки нужно приводить к числовому виду\n",
    "\n",
    "Нужно указать, какое число является \"пропуском\"\n",
    "\n",
    "При сплите, алгоритм смотрит в какую сторону лучше отвести объекты с пропуском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain_missed = xgb.DMatrix(X[test_index], label=y[test_index], missing=-999.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитывает сколько раз каждый признак использовался для использовался в вершине дерева при разбиении\n",
    "\n",
    "Это не качество фактора, а его важность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f12': 13, 'f5': 14, 'f7': 10, 'f11': 2, 'f4': 9, 'f0': 7, 'f10': 7, 'f9': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#постройте график важностей признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прунинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно GBM перестает разделять вершины дерева, когда gain становится отрицательным - жадный подход.\n",
    "Могло оказаться так, что после неудачного сплита с отрицательным gain'ом получится сделать сильно положительный сплит.\n",
    "\n",
    "XGBoost доводит деревья до max_depth, после чего начинает удалять сплиты, которые несут отрицательный вклад."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: в качестве начального приближения можно выбрать предсказания линейной модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенная кросс валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.cv(#параметры,\n",
    "    #обучающая выборка,\n",
    "    nfold = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - regularization\n",
    "  - subsampling\n",
    "  - shrinkage\n",
    "  - early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Параметры xgb.train, необходимые для early stopping\n",
    "   - early_stopping_rounds\n",
    "   - evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Веса для объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем учитывать каждый объект со своим весом, этот вес будет учитываться и при выборе бакетов при приближенном построении деревьев, при сплите, при подсчете Objective.\n",
    "\n",
    "Допустим, мы хотим классифицировать короткие сообщения.  Некоторые из них повторяются. В этом случае выгодно \"слить\" вместе все дубликаты и посчитать их один раз, но с большим весом. При неизменном качестве это уменьшит время обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = np.random.randint(low=1, high=5, size=X.shape[0])\n",
    "train_examples = 300\n",
    "\n",
    "\n",
    "X_train = X[:train_examples]\n",
    "X_test = X[train_examples:]\n",
    "y_train = y[:train_examples]\n",
    "y_test = y[train_examples:]\n",
    "\n",
    "\n",
    "X_train_repeated = np.repeat(X_train, repeats[:train_examples], axis=0)\n",
    "X_test_repeated = np.repeat(X_test, repeats[train_examples:], axis=0)\n",
    "y_train_repeated = np.repeat(y_train, repeats[:train_examples], axis=0)\n",
    "\n",
    "\n",
    "xgtrain_repeated = xgb.DMatrix(X_train_repeated, label=y_train_repeated)\n",
    "xgtrain_weighted = xgb.DMatrix(X_train, label=y_train, weight=repeats[:train_examples])\n",
    "\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "bst = #обучите xgb на xgtrain_repeated\n",
    "print (\"Repeated dataset. Train size: {}, error: {}\".format(xgtrain_repeated.num_row(), bst.eval(xgtest)))\n",
    "\n",
    "bst = #обучите xgb на xgtrain_weighted\n",
    "print (\"Weighted dataset. Train size: {}, error: {}\".format(xgtrain_weighted.num_row(), bst.eval(xgtest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 вида бустеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gbtree - обычные решающие деревья\n",
    "\n",
    "* gblinear - линейные модели\n",
    "\n",
    "* dart - решающие деревья, алгоритм может \"выбрасывать\" некоторые из деревьев, уменьшая переобучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Алгоритм\tViola-Jones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обнаружение объектов (обычно лиц) в реальном времени\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - различных признаков очень много\n",
    "  - используется вариация AdaBoost'a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> learning_rates </i> - можно настроить убывающую скорость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> max_depth </i> - максимальная глубина дерева. Слишком большая глубина ведет к переобучению\n",
    "\n",
    "<i> subsample, colsample_bytree, colsample_bylevel </i> - сэмплирование по объектам и признакам\n",
    "\n",
    "\n",
    "<i> min_child_weight </i> - минимальная сумма весов в листе\n",
    "\n",
    "<i> scale_pos_weight </i> - вес целого класса, используется если один класс заметно чаще встречается, чем другой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные параметры для DART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> sample_type </i> - стратегия выбора деревьев для выкидывания\n",
    "\n",
    "<i> rate_drop </i> - какую долю выкидываем\n",
    "\n",
    "<i> skip_drop </i> - шанс пропустить дроп на этой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настраиваем XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "* Выбираем относительно большую learning_rate ($ \\eta \\in [0.05, 0.3]$), подбираем оптимальное число деревьев для выбранного $ \\eta $\n",
    "\n",
    "* Настраиваем параметры деревьев, начиная с самых значимых (max_depth, min_child_weight, gamma, subsample, colsample_bytree)\n",
    "\n",
    "* Настраиваем регуляризации ($ \\lambda, \\alpha $)\n",
    "\n",
    "* Уменьшаем learning_rate, пропорционально увеличиваем число деревьев"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
